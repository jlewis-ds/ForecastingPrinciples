---
title: "Forecasting Principles and Practice"
author: "Justin Lewis"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Import, echo=FALSE, message=FALSE, warning=FALSE}
library(fpp2)
```
## **Intro**

#### What can be forecast?
Predictability depends on: how well we understand contributing factors, how much data is available, and whether the forecasts themselves affect the thing we forecast.

A key step is knowing when something can be forecast accurately, and when we can do better than flipping a coin. Good forecasts capture genuine patterns and relationships which exist, but do not replicate events that will not occur again. 

#### **5 Basic Steps**

1) Problem Definition

2) Gathering Information

3) Preliminary Exploratory Analysis

4) Choosing and Fitting Models

5) Use and Evaluation of a Forecasting Model

## **Time Series Graphics**

```{r}
#5 years of data, starts in 2012
y <- ts(c(123, 39, 78, 52, 110), start=2012)

#Can also adjust the 'frequency' if data is in a vector.
#This defines the number before the seasonal pattern repeats

#Complicated TS plot using autoplot
autoplot(melsyd[, "Economy.Class"])+ggtitle("Economy Passengers: MEL-SYD")+xlab("Year")+ylab("Thousands")
```

This plot has several more complicated features. Missing data, no passengers for a period, variation in the fluctuations.

```{r}
autoplot(a10)+ggtitle("Drug Sales")+ylab('$ Million')+xlab('Year')
```

Here there's a clear annual increasing trend with increasing seasonal patterns. Defining some simple terms:

-Trend: trends exists when there is a long-term increase or decrease in the data, which does not have to be linear

-Seasonal: a pattern occuring when a series is affected by factors such as time of year or day of the week. This is a fixed, known frequency

-Cyclic: a cycle is when the data rises and falls but is not part of a fixed frequency, sometimes due to economic conditions or business cycles

Many series will include multiple combinations of all three.

#### **Seasonal Plots**

These plot the data against individual 'seasons' in the data, which allows us to identify abnormalities. A variation of this is the polar coordinate seasonal plot, which can be seen in the second plot.

```{r}
ggseasonplot(a10, year.labels=TRUE, year.labels.left=TRUE) + ylab("$ Million")+ggtitle("Seasonal Plot: Drug Sales")


ggseasonplot(a10, polar=TRUE)+ylab('$ Million')+ggtitle('Polar Seasonal Plot: Drug Sales')

```

#### **Seasonal Subseries Plots**

These still emphasize the seasonal patterns but separate each season in separated mini-plots. The one shown below has a bar denoting the average, and shows the monthly trends over time.

```{r}
ggsubseriesplot(a10)+ylab('$ Millions')+ggtitle('Seasonal Subseries Plot: Drug Sales')

```

#### **Scatterplots**

Scatterplots are useful for visualizing the relationships between time series, as well as the values within a series to examine correlations.

```{r}
autoplot(elecdemand[,c("Demand", "Temperature")], facets=TRUE)+xlab('Year: 2014')+ggtitle('Half-hourly Electricity Demand: Victoria, Austrailia')+ylab('')

qplot(Temperature, Demand, data=as.data.frame(elecdemand))+ylab("Demand (GW)")+xlab("Temperature (C)")

```

When examining correlations it is important to keep in mind it only measures the strength of the linear relationship between two variables, and can be misleading. It is possible for very different distributions to have the same correlation coefficient which is why understanding the variables and relationships is so important.

To look at relationships between multiple combinations of variables, we can use a scatterplot matrix:

```{r}
GGally::ggpairs(as.data.frame(visnights[,1:5]))

```

#### **Autocorrelation**

This measures the linear relationship between lagged values of a time series. So how is y_t related to y_t-1 and so on. The strength of these values can be found in R:

```{r}
beer2 <- window(ausbeer, start=1992)
ggAcf(beer2)

```

This shows that there is high autocorrelation with a period of 4 quarters. So r4 is higher since 4th quarter sales are high, and there is lower r2 since troughs tend to be 2 quarters behind peaks. Since these spikes raise above the blue dotted lines, it indicates that they are significantly different than zero.

When data has a trend, the autocorrelations for small lags tend to be large and positive since nearby observations are also similiarly sized. The ACF of a trended series will tend to have positive values that slowly decrease with larger lag values. With both a trend AND seasonal data, there will be a combination of the effects.

```{r}
aelec <- window(elec, start=1980)
autoplot(aelec)+xlab('Year')+ylab('GWh')
ggAcf(aelec, lag=48)

```

Any time series with no autocorrelation at all is called white noise. For white noise we expect the ac to be close to zero, but it will not often be exactly zero simply due to some random variation. We expect 95% of the random spikes to be within +/- 2/sqrt(T) where T is the length of the time series. If any spikes are outside of this bound or if substantially more than 5% are outside the bounds, a series is probably not white noise.

#### **Exercises**

1. Explore gold, woolyrnq, and gas datasets and then a) plot each of them with autoplot(), b) find the frequency of each, and c) use which.max() to spot the outlier in 'gold'.

```{r}
#par(mfrow=c(1,3))
autoplot(gold)+ylab('Price of Gold')+ggtitle('Daily Gold Prices ($)')
frequency(gold)
autoplot(woolyrnq)+ylab('Tonnes of Yarn')+ggtitle('Quarterly Yarn Production')
frequency(woolyrnq)
autoplot(gas)+ylab('Gas Produced')+ggtitle('Monthly Gas Production')
frequency(gas)

which.max(gold)
```

5. Use the ggseasonplot() and ggsubseriesplot() functions to explore seasonal patterns in writing, fancy, h02. What can be said about seasonal patterns? Are there any unusual years?

```{r}
ggseasonplot(writing)
ggsubseriesplot(writing)

#What is going on in August??
```

```{r}
ggseasonplot(fancy)
ggsubseriesplot(fancy)

#Trending up over the years, but clearly a huge spike in their peak summer.
```

```{r}
ggseasonplot(h02)
ggsubseriesplot(h02)

```

10. dj contains 292 trading days of the Dow Jones Index. Compute the daily changes in the index. Do the changes look like white noise?

```{r}
ddj <- diff(dj)
autoplot(ddj)
ggAcf(ddj)

#This looks a lot like white noise, but there is a single lag spike which crosses the boundary, and 3 others which are close. Maybe not?
```

## **The Forecaster's Toolbox**

Coverage of general tools which are useful for different situations in forecasting. Methods to make a task simpler, checking methods for if available information has been adequately utilized, and techniques for computing prediction intervals.

#### **Simple Methods**

While it may be attractive to work on more complicated methods immediately, some very simple methods can be surprisingly effective. Even if they're not the final model approach, they can be used effectively as benchmarks.

* 1) Average Method - All forecasted values are set equal to the mean of the historical data

* 2) Naive Method - All forecasted values are set equal to the most recent data point observed. This works particularly well when data follows a random walk, so it can also be called a 'random walk' forecast.

* 3) Seasonal Naive Method - For highly seasonal data, we set each forecast to be equal to the last observed value from the same season of the year. For example, set next year's Feb forecast value to be the same as this year's past Feb observed value. If forecasting Q2 values next year, reuse Q2 observation from this year.

* 4) Drift Method - This allows the forecasts to increase/decrease over time, where the amount of change over time (drift) is set to the average change seen in all historical data. Equivalent to drawing a line from first obs to last, and extrapolating it into the future.

These can be seen applied to seasonal and non-seasonal data below.

```{r}
#1992 to 2007 training data.
beer2 <- window(ausbeer, start=1992, end=c(2007,4))

autoplot(beer2)+
  autolayer(meanf(beer2, h=11), series='Mean', PI=FALSE)+
  autolayer(naive(beer2, h=11), series='Naive', PI=FALSE)+
  autolayer(snaive(beer2, h=11), series='Seasonal', PI=FALSE)+ggtitle('Forecasts for Quarterly Beer Production')+xlab('Year')+ylab('MLs')+
  guides(colour=guide_legend(title='Forecast'))


autoplot(goog200) +
  autolayer(meanf(goog200, h=40),
    series="Mean", PI=FALSE) +
  autolayer(rwf(goog200, h=40),
    series="Naïve", PI=FALSE) +
  autolayer(rwf(goog200, drift=TRUE, h=40),
    series="Drift", PI=FALSE) +
  ggtitle("Google stock (daily ending 6 Dec 2013)") +
  xlab("Day") + ylab("Closing Price (US$)") +
  guides(colour=guide_legend(title="Forecast"))
```

There will be times where one of these simple methods is the best available method, but most often they will serve as benchmarks to compare to rather than the chosen method.

#### **Transformations and Adjustments**

By adjusting historical data we can often simplify the forecasting task at hand. Adjustments and transformations can simplify the patterns in the historical data by removing known sources of variation. Simpler patterns lead to more accurate forecasts.

##### Calender Adjustments

This has to do with variation simply due to the natural differences in lengths of months or quarters. 

```{r}
dframe <- cbind(Monthly = milk, DailyAverage = milk/monthdays(milk))
autoplot(dframe, facet=TRUE)+
  xlab("Years")+
  ylab("Pounds")+
  ggtitle('Milk Production Per Cow')
```

We can see that the seasonal pattern is significantly more simple when we are using a daily average instead of the total without considering month length.

##### Population Adjustments

In this case, any data which may be affected by population changes can be adjusted to give per-capita data instead. Consider the metric per thousand or per million people.

##### Inflation Adjustments

Data which is affected by the value of money should also be adjusted to account for inflation before attempting to model it. For this we define values as dollar values from a given year (year 2000 for example). A price index is used for these adjustments, commonly the Consumer Price Index is used for consumer goods.

##### Mathematical Transformations

If data shows variation which increases/decreases along with the level of the series then a transformation, such as a log() transform can be useful. Log transforms are useful because they are both interpretable, and they constrain forecasts to stay positive on the original scale.

Other transformations can also be used however they are not as interpretable, such as power transformations. A useful combination of the two is the family of Box-Cox transformations which depend on an adjustable parameter $\lambda$. These are defined as:

\begin{equation}
w_t = \begin{cases}
  log(y_t) & \text{if} \quad \lambda = 0 \\
  (y_t^\lambda -1)/\lambda & \quad \text{otherwise}
  \end{cases}
\end{equation}

If $\lambda = 1$ then $w_t = y_t - 1$ and the transformed data is shifted downwards but there is no change in shape. For all other values of $\lambda$ the time series data changes shape. A good value if one which makes the size of the seasonal variation approximately the same across the entire series.

```{r}
#This BoxCox.lambda call will choose a likely value for us
lambda <- BoxCox.lambda(elec)
lambda

autoplot(BoxCox(elec,lambda))

#But we can also see the result of choosing a significantly different one

autoplot(BoxCox(elec, 0.7))

#And the data without the transformation
autoplot(elec)

```

With the transformation chosen, we need to reverse the transformation to obtain our actual forecasts on the original scale.

\begin{equation}
y_t = \begin{cases}
  exp(w_t) & \text{if} \quad \lambda = 0 \\
  (\lambda w_t + 1)^{\frac{1}{\lambda}} & \quad \text{otherwise}
  \end{cases}
\end{equation}

Some important features to consider:

* if some $y_t < 0$ no power transformation is possible unless all obs are adjusted by adding a constant

* forecasting results are relatively insensitive to lambda

* transforms can sometimes make little difference to
forecasted values, but have large effects on prediction intervals

##### Bias Adjustments

When using these transformations, it's important to note that the back-transformed point forecast will not be in the **mean** of the forecast distribution, but more often the **median**. Often this will be just fine, but occasionally the mean will be required. For instance, adding up sales forecasts across multiple regions. Medians cannot be added up correctly, but the means will.

Bias adjustment is not done by default in forecast package in R, so if this is desiredthe flag $biasadj=TRUE$ should be set when choosing Box-Cox transformation parameter.

#### **Residual Diagnostics**

For many, but not *all*, time series models the residuals are equal to the difference between the obs and the corresponding fitted values:

\begin{equation}
e_t = y_t - \hat{y_t}
\end{equation}

Residuals are useful for checking whether a model has adequately captured the information in the data, and a good forecasting method will have residuals with the properties:

* residuals are uncorrelated. If there is correlation then there is information left in the residuals which should be used in the forecasts

* residuals have zero mean. If the residuals have a non-zero mean then they are *biased*

If a forecasting method does not satisfy these properties then it can certainly be improved. However, that does not mean forecasting methods which **do** satisfy them cannot be improved. These properties are useful for seeing if a method is using all available information, but are not effective for method selection.

If either property is not satisfied then the method can be modified to give better forecasts. Bias adjustments are simple, just add the mean of the residuals to all forecasts. Fixing correlation is more difficult and discussed in depth later on.

Additional useful (but not required) properties of the residuals are:

* residuals have constant variance

* residuals are normally distributed

These properties make prediction interval calculation easier, however they do not indicate whether a method can be improved or not.

```{r Google Example}

autoplot(goog200)+ylab('Closing Price (USD)')+
  ggtitle('Google Stock (Daily)')

res <- residuals(naive(goog200))
autoplot(res)+xlab('Day')+ylab('')+
  ggtitle('Residuals from Naive Method')
#Is the mean zero?

gghistogram(res)+
  ggtitle('Histogram of Residuals')
#Are they normally distributed?

ggAcf(res)+
  ggtitle('ACF of Residuals')

```

Based on the discussion above, these graphs appear to show that the naive method is accounting for all available information. The mean is near zero, and there's no significant correlation in the series of residuals. However the histogram does suggest that they may not be normal, even if we ignore the outlier point the right tail is a bit too long. So the forecasts may be fairly good, however prediction intervals computed assuming a normal distribution may be inaccurate.

##### Portmanteau Tests for Autocorrelation

For a more formal test of autocorrelation, we can considers an entire set of $r_k$ values where $r_k$ is the autocorrelation ofr lag $k$. The ACF plot is implicitly carrying out multiple hypothesis tests, and if we consider enough lag points then it becomes likely that at least one will return a false positive. 

To overcome this we can test whether the first $h$ autocorrelations are significantly different from what we would usually see in a white noise process. A test for a group of autocorrelations is called a *portmanteau test*.

One of these tests is the **Box-Pierce Test** based on:

\begin{equation}

Q = T \sum_{k=1}^h r_k^2

\end{equation}

Here $h$ is the maximum lag being considered, while $T$ is the number of observations. If the autocorrelations are all close to zero, then we expect $Q$ to be small. Otherwise it will be large. For non-seasonal data $h = 10$ is recommended, while $h = 2m$ is recommended for seasonal data with period $m$. However this test is not good when $h$ is large, so if either of these recommendations are larger than $T/5$, use $h=\frac{T}{5}$.

For a more accurate test, we turn to the **Ljung-Box Test** which uses:

\begin{equation}
Q^* = T(T+2)\sum_{k=1}^h(T-k)^{-1}r_k^2
\end{equation}

where large $Q^*$ suggests autocorrelation exists.

To determine what large is, we consider that if looking at a white noise series then both $Q$ and $Q^*$ would have a $\Chi^2$ distribution with (h-K) degrees of freedom. With this knowledge we can calculate p-values for the values of $Q$ and $Q^*$ to determine if they are significant.

```{r Residual Tests}
Box.test(res, lag=10, fitdf=0)
Box.test(res, lag=10, fitdf=0, type='Lj')

#Use all methods with convenient call...
checkresiduals(naive(goog200))

```

#### **Evaluating Forecast Accuracy**

Since the data used to fit the method and create the residuals uses the data itself, they will not be a good indicator of how well the forecast actually does. To determine this we need to forecast new data that has not been seen before. This leads to the common practice of creating train/test splits, to create and tune the forecast model with the training data, then evaluate the true performance by forecasting over the same time period as the test data then comparing how close to the test data our forecast predictions are.

Useful functions in R for subsetting data are shown in the following section.

```{r Subsetting Functions}
#Extract data from 1995 onwards
window(ausbeer, start=1995)

#Subset based on indices. Grab the past 5 years of data (because it is stored as quarters)
subset(ausbeer, start=length(ausbeer)-4*5)

#Also grab specific quarter for every year
subset(ausbeer, quarter = 1)

#There is also head() and tail() 
```

##### **Forecast Errors**

These are the difference between the observed value in the test set, and the forecasted value. *Error* here does not necessarily mean mistake, rather it means the unpredictable part of an observation, expressed as:

\begin{equation}
e_{T+h} = y_{T+h} - \hat{y_{T+h|T}}
\end{equation}

These are different than residuals because they are calculated on the test set, and they can involve multi-step forecasts into the future while residuals only consider a single value.

Scale dependent errors are errors which are on the same scale as the data. Measures based only on $e_t$ are therefore scale dependent and cannot be used to compare series with different time units. Two common measures:

* MAE = mean($|e_t|$)

* RMSE = $\sqrt{(\text{mean}(e_t^2))}$

Percentage errors have the advantage of being unit free, so can be used to compare forecast performances across different datasets. Most commonly used is MAPE:

* MAPE = mean($|p_t|)$

where $p_t = \frac{100e_t}{y_t}$. However, these have the issue of going to infinity/undefined values if there is any chance $y_t = 0$. Another disadvantage is that they disproportionately penalize negative errors over positive errors. This leads to the 'symmetric' MAPE, but this sMAPE also has several issues associated with it and the authors advise against using it.

To address the shortcomings of percentage errors, **scaled errors** were proposed as an alternative. These scale errors based on training MAE from a simpler forecasting method. For instance if we use naive forecasts for the training MAE:

* Non-seasonal: $q_j = \frac{e_j}{\frac{1}{T-1}sum_{t=2}^T|y_t - y_{t-1}|}

* Seasonal: $q_j = \frac{e_j}{\frac{1}{T-m}sum_{t=m+1}^T|y_t - y_{t-m}|}

A scaled error is less than 1 if it arises from a *better* forecast than the average naive method computed on the training data, and greater than 1 if it arises from a *worse* forecast. The mean absolute scaled error is simply calculated as MASE = mean($|q_j|$). 

```{r Forecast Accuracy Examples}
#We can compare the approaches and what the error methods tell us
beer2 <- window(ausbeer,start=1992,end=c(2007,4))
beerfit1 <- meanf(beer2,h=10)
beerfit2 <- rwf(beer2,h=10)
beerfit3 <- snaive(beer2,h=10)
autoplot(window(ausbeer, start=1992)) +
  autolayer(beerfit1, series="Mean", PI=FALSE) +
  autolayer(beerfit2, series="Naive", PI=FALSE) +
  autolayer(beerfit3, series="Seasonal naïve", PI=FALSE) +
  xlab("Year") + ylab("Megalitres") +
  ggtitle("Forecasts for quarterly beer production") +
  guides(colour=guide_legend(title="Forecast"))

beer3 <- window(ausbeer, start=2008)
accuracy(beerfit1, beer3)
accuracy(beerfit2, beer3)
accuracy(beerfit3, beer3)

```

We can tell from the plot that seasonal naive performs the best, but in this case all metrics also agree with this assessment. Sometimes this may not be true or the best model may not be so easily identifiable. 

##### **Time Series - Cross Validation**

This is a more sophisticated version of train/test splitting, where multiple sets are considered to reduce the likelihood of overfitting to a specific subset of data in the training portion. However with time-series data this means we need to consider only data before a certain point, then create n-step future forecasts for a single observation. We then take an average of the accuracies across all the subsets. This is evaluation on a *rolling* forecasting origin, since we roll forward in time as we create new training sets. And since a single step forward may not be the most interesting to us, we can create forecasts for n-steps into the future.

```{r TS Cross Validation}
#Root mean squared error calculation
e <- tsCV(goog200, rwf, drift=TRUE, h=1)
sqrt(mean(e^2, na.rm=TRUE))

#Residual RMSE calculation
sqrt(mean(residuals(rwf(goog200,drift=TRUE))^2, na.rm=TRUE))

```

Of course the RMSE of the residuals is smaller, since it is using all the data available while the true forecasts are not. One good approach to choose the best model is to find the model with the smallest forecast RMSE computed with time series cv.

##### Pipe Operator Introduction

The code above can be a bit difficult to read, since we work from the inside out if we want to truly understand what is happening since there are nested functions. To make more legible code we can use the pipe operator. The LHS gets fed into the function to the right of the pipe. This makes it simple to walk through the code instead of having to parse nested functions.

```{r Pipe}
goog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1) -> e
e^2 %>% mean(na.rm=TRUE) %>% sqrt()
#> [1] 6.233
goog200 %>% rwf(drift=TRUE) %>% residuals() -> res
res^2 %>% mean(na.rm=TRUE) %>% sqrt()
```